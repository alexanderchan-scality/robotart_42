{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "This notebook contains code to train a linear classifier on MNIST. At the end is a short exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import scipy\n",
    "import scipy.misc\n",
    "import math\n",
    "import PIL\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import re\n",
    "import random\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "path = './images/'\n",
    "\n",
    "sess = None\n",
    "def ResetSession():\n",
    "    tf.reset_default_graph()\n",
    "    global sess\n",
    "    if sess is not None: sess.close()\n",
    "    sess = tf.InteractiveSession()\n",
    "ResetSession()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "NUM_CLASSES = 400\n",
    "NUM_PIXELS = 200 * 200\n",
    "TRAIN_STEPS = 1000\n",
    "BATCH_SIZE = 100\n",
    "LEARNING_RATE = 0.5\n",
    "\n",
    "TEST_SIZE = 1000\n",
    "TRAIN_SIZE = 4000\n",
    "\n",
    "HIDDEN_1 = 60\n",
    "HIDDEN_2 = 6\n",
    "\n",
    "NUM_RES = 1\n",
    "\n",
    "MAX_VAL = 255\n",
    "\n",
    "whole_set = glob.glob(path + '*.png')\n",
    "random.shuffle(whole_set)\n",
    "test_set = whole_set[:TEST_SIZE]\n",
    "train_set = whole_set[TEST_SIZE:TRAIN_SIZE + TEST_SIZE]\n",
    "whole_set = []\n",
    "\n",
    "train_labels = []\n",
    "train_data = []\n",
    "for i in train_set:\n",
    "    regex_res = re.search('X([0-9]*\\.?[0-9]+)_Y([0-9]*\\.?[0-9]+)(_X([0-9]*\\.?[0-9]+)_Y([0-9]*\\.?[0-9]+))?.png', i)\n",
    "    \n",
    "    append_label = np.zeros((NUM_RES), dtype=np.float32)\n",
    "    x_ind = (int(regex_res.group(1)))\n",
    "    np.put(append_label, 0, x_ind)\n",
    "    train_labels.append(append_label)\n",
    "    \n",
    "    img = scipy.misc.imread(i, flatten=False, mode='RGBA').astype(np.float)\n",
    "    img = scipy.misc.imresize(img, 50)\n",
    "    img = img[:,:,3]\n",
    "    img.shape = (NUM_PIXELS)\n",
    "    img = img / MAX_VAL\n",
    "#     print (np.amin(img), np.amax(img))\n",
    "#     train_data.append(img)\n",
    "    train_data.append(img)\n",
    "#     break\n",
    "train_labels = np.array(train_labels)\n",
    "train_data = np.array(train_data)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 loss 53422.156250\n",
      "1 loss 607824.625000\n",
      "2 loss 54168.707031\n",
      "3 loss 53447.335938\n",
      "4 loss 53590.636719\n",
      "5 loss 53622.320312\n",
      "6 loss 53589.398438\n",
      "7 loss 53522.746094\n",
      "8 loss 53451.296875\n",
      "9 loss 53356.597656\n",
      "10 loss 53264.000000\n",
      "11 loss 53166.281250\n",
      "12 loss 53061.253906\n",
      "13 loss 52949.859375\n",
      "14 loss 52832.890625\n",
      "15 loss 52711.023438\n",
      "16 loss 52584.824219\n",
      "17 loss 52454.804688\n",
      "18 loss 52321.371094\n",
      "19 loss 52184.914062\n",
      "20 loss 52045.773438\n",
      "21 loss 51904.218750\n",
      "22 loss 51760.515625\n",
      "23 loss 51614.898438\n",
      "24 loss 51467.570312\n",
      "25 loss 51318.714844\n",
      "26 loss 51168.507812\n",
      "27 loss 51017.085938\n",
      "28 loss 50864.585938\n",
      "29 loss 50711.136719\n",
      "30 loss 50556.839844\n",
      "31 loss 50401.812500\n",
      "32 loss 50246.128906\n",
      "33 loss 50089.886719\n",
      "34 loss 49933.156250\n",
      "35 loss 49776.007812\n",
      "36 loss 49618.511719\n",
      "37 loss 49460.710938\n",
      "38 loss 49302.683594\n",
      "39 loss 49144.464844\n",
      "40 loss 48986.097656\n",
      "41 loss 48827.632812\n",
      "42 loss 48669.113281\n",
      "43 loss 48510.562500\n",
      "44 loss 48352.031250\n",
      "45 loss 48193.539062\n",
      "46 loss 48035.113281\n",
      "47 loss 47876.777344\n",
      "48 loss 47718.570312\n",
      "49 loss 47560.507812\n",
      "50 loss 47402.613281\n",
      "51 loss 47244.894531\n",
      "52 loss 47087.390625\n",
      "53 loss 46930.105469\n",
      "54 loss 46773.054688\n",
      "55 loss 46616.265625\n",
      "56 loss 46459.734375\n",
      "57 loss 46303.496094\n",
      "58 loss 46147.542969\n",
      "59 loss 45991.894531\n",
      "60 loss 45836.570312\n",
      "61 loss 45681.562500\n",
      "62 loss 45526.898438\n",
      "63 loss 45372.589844\n",
      "64 loss 45218.617188\n",
      "65 loss 45065.015625\n",
      "66 loss 44911.781250\n",
      "67 loss 44758.929688\n",
      "68 loss 44606.449219\n",
      "69 loss 44454.359375\n",
      "70 loss 44302.671875\n",
      "71 loss 44151.386719\n",
      "72 loss 44000.507812\n",
      "73 loss 43850.039062\n",
      "74 loss 43699.984375\n",
      "75 loss 43550.355469\n",
      "76 loss 43401.148438\n",
      "77 loss 43252.375000\n",
      "78 loss 43104.031250\n",
      "79 loss 42956.125000\n",
      "80 loss 42808.656250\n",
      "81 loss 42661.632812\n",
      "82 loss 42515.054688\n",
      "83 loss 42368.929688\n",
      "84 loss 42223.246094\n",
      "85 loss 42078.027344\n",
      "86 loss 41933.261719\n",
      "87 loss 41788.957031\n",
      "88 loss 41645.113281\n",
      "89 loss 41501.726562\n",
      "90 loss 41358.808594\n",
      "91 loss 41216.355469\n",
      "92 loss 41074.375000\n",
      "93 loss 40932.863281\n",
      "94 loss 40791.824219\n",
      "95 loss 40651.253906\n",
      "96 loss 40511.164062\n",
      "97 loss 40371.546875\n",
      "98 loss 40232.402344\n",
      "99 loss 40093.742188\n",
      "100 loss 39955.554688\n",
      "101 loss 39817.851562\n",
      "102 loss 39680.628906\n",
      "103 loss 39543.890625\n",
      "104 loss 39407.625000\n",
      "105 loss 39271.851562\n",
      "106 loss 39136.558594\n",
      "107 loss 39001.742188\n",
      "108 loss 38867.414062\n",
      "109 loss 38733.574219\n",
      "110 loss 38600.222656\n",
      "111 loss 38467.351562\n",
      "112 loss 38334.968750\n",
      "113 loss 38203.070312\n",
      "114 loss 38071.660156\n",
      "115 loss 37940.730469\n",
      "116 loss 37810.292969\n",
      "117 loss 37680.339844\n",
      "118 loss 37550.875000\n",
      "119 loss 37421.894531\n",
      "120 loss 37293.402344\n",
      "121 loss 37165.394531\n",
      "122 loss 37037.875000\n",
      "123 loss 36910.847656\n",
      "124 loss 36784.300781\n",
      "125 loss 36658.242188\n",
      "126 loss 36532.664062\n",
      "127 loss 36407.574219\n",
      "128 loss 36282.976562\n",
      "129 loss 36158.859375\n",
      "130 loss 36035.222656\n",
      "131 loss 35912.070312\n",
      "132 loss 35789.410156\n",
      "133 loss 35667.226562\n",
      "134 loss 35545.535156\n",
      "135 loss 35424.316406\n",
      "136 loss 35303.589844\n",
      "137 loss 35183.335938\n",
      "138 loss 35063.566406\n",
      "139 loss 34944.273438\n",
      "140 loss 34825.472656\n",
      "141 loss 34707.140625\n",
      "142 loss 34589.292969\n",
      "143 loss 34471.921875\n",
      "144 loss 34355.027344\n",
      "145 loss 34238.617188\n",
      "146 loss 34122.675781\n",
      "147 loss 34007.214844\n",
      "148 loss 33892.222656\n",
      "149 loss 33777.714844\n",
      "150 loss 33663.671875\n",
      "151 loss 33550.105469\n",
      "152 loss 33437.011719\n",
      "153 loss 33324.394531\n",
      "154 loss 33212.242188\n",
      "155 loss 33100.562500\n",
      "156 loss 32989.351562\n",
      "157 loss 32878.605469\n",
      "158 loss 32768.332031\n",
      "159 loss 32658.521484\n",
      "160 loss 32549.179688\n",
      "161 loss 32440.304688\n",
      "162 loss 32331.886719\n",
      "163 loss 32223.935547\n",
      "164 loss 32116.449219\n",
      "165 loss 32009.423828\n",
      "166 loss 31902.865234\n",
      "167 loss 31796.755859\n",
      "168 loss 31691.109375\n",
      "169 loss 31585.927734\n",
      "170 loss 31481.197266\n",
      "171 loss 31376.923828\n",
      "172 loss 31273.101562\n",
      "173 loss 31169.738281\n",
      "174 loss 31066.824219\n",
      "175 loss 30964.365234\n",
      "176 loss 30862.363281\n",
      "177 loss 30760.804688\n",
      "178 loss 30659.691406\n",
      "179 loss 30559.031250\n",
      "180 loss 30458.820312\n",
      "181 loss 30359.052734\n",
      "182 loss 30259.728516\n",
      "183 loss 30160.849609\n",
      "184 loss 30062.419922\n",
      "185 loss 29964.425781\n",
      "186 loss 29866.871094\n",
      "187 loss 29769.761719\n",
      "188 loss 29673.095703\n",
      "189 loss 29576.863281\n",
      "190 loss 29481.058594\n",
      "191 loss 29385.699219\n",
      "192 loss 29290.769531\n",
      "193 loss 29196.275391\n",
      "194 loss 29102.216797\n",
      "195 loss 29008.587891\n",
      "196 loss 28915.390625\n",
      "197 loss 28822.615234\n",
      "198 loss 28730.277344\n",
      "199 loss 28638.357422\n",
      "200 loss 28546.871094\n",
      "201 loss 28455.808594\n",
      "202 loss 28365.166016\n",
      "203 loss 28274.947266\n",
      "204 loss 28185.152344\n",
      "205 loss 28095.771484\n",
      "206 loss 28006.814453\n",
      "207 loss 27918.275391\n",
      "208 loss 27830.152344\n",
      "209 loss 27742.443359\n",
      "210 loss 27655.152344\n",
      "211 loss 27568.269531\n",
      "212 loss 27481.806641\n",
      "213 loss 27395.748047\n",
      "214 loss 27310.097656\n",
      "215 loss 27224.859375\n",
      "216 loss 27140.031250\n",
      "217 loss 27055.607422\n",
      "218 loss 26971.587891\n",
      "219 loss 26887.970703\n",
      "220 loss 26804.763672\n",
      "221 loss 26721.949219\n",
      "222 loss 26639.539062\n",
      "223 loss 26557.523438\n",
      "224 loss 26475.916016\n",
      "225 loss 26394.701172\n",
      "226 loss 26313.876953\n",
      "227 loss 26233.451172\n",
      "228 loss 26153.417969\n",
      "229 loss 26073.781250\n",
      "230 loss 25994.533203\n",
      "231 loss 25915.671875\n",
      "232 loss 25837.203125\n",
      "233 loss 25759.115234\n",
      "234 loss 25681.416016\n",
      "235 loss 25604.105469\n",
      "236 loss 25527.173828\n",
      "237 loss 25450.632812\n",
      "238 loss 25374.466797\n",
      "239 loss 25298.683594\n",
      "240 loss 25223.275391\n",
      "241 loss 25148.246094\n",
      "242 loss 25073.597656\n",
      "243 loss 24999.320312\n",
      "244 loss 24925.416016\n",
      "245 loss 24851.890625\n",
      "246 loss 24778.730469\n",
      "247 loss 24705.941406\n",
      "248 loss 24633.523438\n",
      "249 loss 24561.474609\n",
      "250 loss 24489.787109\n",
      "251 loss 24418.470703\n",
      "252 loss 24347.517578\n",
      "253 loss 24276.929688\n",
      "254 loss 24206.699219\n",
      "255 loss 24136.833984\n",
      "256 loss 24067.324219\n",
      "257 loss 23998.175781\n",
      "258 loss 23929.382812\n",
      "259 loss 23860.943359\n",
      "260 loss 23792.865234\n",
      "261 loss 23725.136719\n",
      "262 loss 23657.759766\n",
      "263 loss 23590.740234\n",
      "264 loss 23524.062500\n",
      "265 loss 23457.738281\n",
      "266 loss 23391.757812\n",
      "267 loss 23326.123047\n",
      "268 loss 23260.839844\n",
      "269 loss 23195.896484\n",
      "270 loss 23131.294922\n",
      "271 loss 23067.031250\n",
      "272 loss 23003.113281\n",
      "273 loss 22939.531250\n",
      "274 loss 22876.285156\n",
      "275 loss 22813.378906\n",
      "276 loss 22750.808594\n",
      "277 loss 22688.570312\n",
      "278 loss 22626.667969\n",
      "279 loss 22565.091797\n",
      "280 loss 22503.849609\n",
      "281 loss 22442.935547\n",
      "282 loss 22382.349609\n",
      "283 loss 22322.087891\n",
      "284 loss 22262.160156\n",
      "285 loss 22202.552734\n",
      "286 loss 22143.265625\n",
      "287 loss 22084.304688\n",
      "288 loss 22025.660156\n",
      "289 loss 21967.335938\n",
      "290 loss 21909.332031\n",
      "291 loss 21851.646484\n",
      "292 loss 21794.273438\n",
      "293 loss 21737.220703\n",
      "294 loss 21680.476562\n",
      "295 loss 21624.048828\n",
      "296 loss 21567.929688\n",
      "297 loss 21512.119141\n",
      "298 loss 21456.617188\n",
      "299 loss 21401.427734\n",
      "300 loss 21346.542969\n",
      "301 loss 21291.960938\n",
      "302 loss 21237.683594\n",
      "303 loss 21183.710938\n",
      "304 loss 21130.035156\n",
      "305 loss 21076.664062\n",
      "306 loss 21023.593750\n",
      "307 loss 20970.822266\n",
      "308 loss 20918.341797\n",
      "309 loss 20866.162109\n",
      "310 loss 20814.273438\n",
      "311 loss 20762.683594\n",
      "312 loss 20711.382812\n",
      "313 loss 20660.373047\n",
      "314 loss 20609.654297\n",
      "315 loss 20559.220703\n",
      "316 loss 20509.080078\n",
      "317 loss 20459.224609\n",
      "318 loss 20409.652344\n",
      "319 loss 20360.363281\n",
      "320 loss 20311.359375\n",
      "321 loss 20262.636719\n",
      "322 loss 20214.197266\n",
      "323 loss 20166.033203\n",
      "324 loss 20118.150391\n",
      "325 loss 20070.544922\n",
      "326 loss 20023.214844\n",
      "327 loss 19976.158203\n",
      "328 loss 19929.376953\n",
      "329 loss 19882.863281\n",
      "330 loss 19836.628906\n",
      "331 loss 19790.660156\n",
      "332 loss 19744.962891\n",
      "333 loss 19699.531250\n",
      "334 loss 19654.367188\n",
      "335 loss 19609.472656\n",
      "336 loss 19564.839844\n",
      "337 loss 19520.472656\n",
      "338 loss 19476.361328\n",
      "339 loss 19432.515625\n",
      "340 loss 19388.929688\n",
      "341 loss 19345.603516\n",
      "342 loss 19302.535156\n",
      "343 loss 19259.722656\n",
      "344 loss 19217.166016\n",
      "345 loss 19174.863281\n",
      "346 loss 19132.816406\n",
      "347 loss 19091.019531\n",
      "348 loss 19049.476562\n",
      "349 loss 19008.179688\n",
      "350 loss 18967.136719\n",
      "351 loss 18926.339844\n",
      "352 loss 18885.787109\n",
      "353 loss 18845.484375\n",
      "354 loss 18805.421875\n",
      "355 loss 18765.605469\n",
      "356 loss 18726.029297\n",
      "357 loss 18686.695312\n",
      "358 loss 18647.603516\n",
      "359 loss 18608.750000\n",
      "360 loss 18570.134766\n",
      "361 loss 18531.753906\n",
      "362 loss 18493.611328\n",
      "363 loss 18455.705078\n",
      "364 loss 18418.031250\n",
      "365 loss 18380.591797\n",
      "366 loss 18343.382812\n",
      "367 loss 18306.404297\n",
      "368 loss 18269.652344\n",
      "369 loss 18233.132812\n",
      "370 loss 18196.837891\n",
      "371 loss 18160.771484\n",
      "372 loss 18124.931641\n",
      "373 loss 18089.312500\n",
      "374 loss 18053.917969\n",
      "375 loss 18018.744141\n",
      "376 loss 17983.794922\n",
      "377 loss 17949.062500\n",
      "378 loss 17914.552734\n",
      "379 loss 17880.255859\n",
      "380 loss 17846.179688\n",
      "381 loss 17812.316406\n",
      "382 loss 17778.669922\n",
      "383 loss 17745.236328\n",
      "384 loss 17712.017578\n",
      "385 loss 17679.007812\n",
      "386 loss 17646.212891\n",
      "387 loss 17613.623047\n",
      "388 loss 17581.246094\n",
      "389 loss 17549.076172\n",
      "390 loss 17517.111328\n",
      "391 loss 17485.353516\n",
      "392 loss 17453.800781\n",
      "393 loss 17422.449219\n",
      "394 loss 17391.304688\n",
      "395 loss 17360.359375\n",
      "396 loss 17329.615234\n",
      "397 loss 17299.068359\n",
      "398 loss 17268.724609\n",
      "399 loss 17238.572266\n",
      "400 loss 17208.623047\n",
      "401 loss 17178.867188\n",
      "402 loss 17149.308594\n",
      "403 loss 17119.939453\n",
      "404 loss 17090.767578\n",
      "405 loss 17061.785156\n",
      "406 loss 17032.996094\n",
      "407 loss 17004.394531\n",
      "408 loss 16975.980469\n",
      "409 loss 16947.755859\n",
      "410 loss 16919.720703\n",
      "411 loss 16891.867188\n",
      "412 loss 16864.203125\n",
      "413 loss 16836.720703\n",
      "414 loss 16809.421875\n",
      "415 loss 16782.306641\n",
      "416 loss 16755.373047\n",
      "417 loss 16728.617188\n",
      "418 loss 16702.044922\n",
      "419 loss 16675.646484\n",
      "420 loss 16649.431641\n",
      "421 loss 16623.390625\n",
      "422 loss 16597.525391\n",
      "423 loss 16571.833984\n",
      "424 loss 16546.318359\n",
      "425 loss 16520.974609\n",
      "426 loss 16495.804688\n",
      "427 loss 16470.802734\n",
      "428 loss 16445.974609\n",
      "429 loss 16421.314453\n",
      "430 loss 16396.822266\n",
      "431 loss 16372.499023\n",
      "432 loss 16348.340820\n",
      "433 loss 16324.348633\n",
      "434 loss 16300.522461\n",
      "435 loss 16276.860352\n",
      "436 loss 16253.361328\n",
      "437 loss 16230.024414\n",
      "438 loss 16206.848633\n",
      "439 loss 16183.833984\n",
      "440 loss 16160.980469\n",
      "441 loss 16138.284180\n",
      "442 loss 16115.745117\n",
      "443 loss 16093.365234\n",
      "444 loss 16071.141602\n",
      "445 loss 16049.072266\n",
      "446 loss 16027.158203\n",
      "447 loss 16005.397461\n",
      "448 loss 15983.790039\n",
      "449 loss 15962.332031\n",
      "450 loss 15941.028320\n",
      "451 loss 15919.874023\n",
      "452 loss 15898.868164\n",
      "453 loss 15878.013672\n",
      "454 loss 15857.303711\n",
      "455 loss 15836.742188\n",
      "456 loss 15816.327148\n",
      "457 loss 15796.057617\n",
      "458 loss 15775.932617\n",
      "459 loss 15755.951172\n",
      "460 loss 15736.113281\n",
      "461 loss 15716.416016\n",
      "462 loss 15696.862305\n",
      "463 loss 15677.448242\n",
      "464 loss 15658.170898\n",
      "465 loss 15639.035156\n",
      "466 loss 15620.037109\n",
      "467 loss 15601.174805\n",
      "468 loss 15582.452148\n",
      "469 loss 15563.861328\n",
      "470 loss 15545.406250\n",
      "471 loss 15527.086914\n",
      "472 loss 15508.900391\n",
      "473 loss 15490.845703\n",
      "474 loss 15472.924805\n",
      "475 loss 15455.131836\n",
      "476 loss 15437.470703\n",
      "477 loss 15419.940430\n",
      "478 loss 15402.538086\n",
      "479 loss 15385.262695\n",
      "480 loss 15368.114258\n",
      "481 loss 15351.092773\n",
      "482 loss 15334.197266\n",
      "483 loss 15317.425781\n",
      "484 loss 15300.780273\n",
      "485 loss 15284.255859\n",
      "486 loss 15267.857422\n",
      "487 loss 15251.581055\n",
      "488 loss 15235.422852\n",
      "489 loss 15219.387695\n",
      "490 loss 15203.470703\n",
      "491 loss 15187.673828\n",
      "492 loss 15171.995117\n",
      "493 loss 15156.433594\n",
      "494 loss 15140.989258\n",
      "495 loss 15125.661133\n",
      "496 loss 15110.449219\n",
      "497 loss 15095.352539\n",
      "498 loss 15080.370117\n",
      "499 loss 15065.500000\n",
      "500 loss 15050.741211\n",
      "501 loss 15036.095703\n",
      "502 loss 15021.561523\n",
      "503 loss 15007.138672\n",
      "504 loss 14992.825195\n",
      "505 loss 14978.623047\n",
      "506 loss 14964.528320\n",
      "507 loss 14950.540039\n",
      "508 loss 14936.661133\n",
      "509 loss 14922.886719\n",
      "510 loss 14909.219727\n",
      "511 loss 14895.658203\n",
      "512 loss 14882.200195\n",
      "513 loss 14868.844727\n",
      "514 loss 14855.595703\n",
      "515 loss 14842.446289\n",
      "516 loss 14829.402344\n",
      "517 loss 14816.457031\n",
      "518 loss 14803.613281\n",
      "519 loss 14790.871094\n",
      "520 loss 14778.225586\n",
      "521 loss 14765.680664\n",
      "522 loss 14753.233398\n",
      "523 loss 14740.883789\n",
      "524 loss 14728.629883\n",
      "525 loss 14716.472656\n",
      "526 loss 14704.412109\n",
      "527 loss 14692.446289\n",
      "528 loss 14680.575195\n",
      "529 loss 14668.795898\n",
      "530 loss 14657.111328\n",
      "531 loss 14645.518555\n",
      "532 loss 14634.017578\n",
      "533 loss 14622.607422\n",
      "534 loss 14611.289062\n",
      "535 loss 14600.064453\n",
      "536 loss 14588.925781\n",
      "537 loss 14577.876953\n",
      "538 loss 14566.914062\n",
      "539 loss 14556.041992\n",
      "540 loss 14545.253906\n",
      "541 loss 14534.555664\n",
      "542 loss 14523.942383\n",
      "543 loss 14513.414062\n",
      "544 loss 14502.971680\n",
      "545 loss 14492.613281\n",
      "546 loss 14482.337891\n",
      "547 loss 14472.146484\n",
      "548 loss 14462.037109\n",
      "549 loss 14452.010742\n",
      "550 loss 14442.065430\n",
      "551 loss 14432.202148\n",
      "552 loss 14422.417969\n",
      "553 loss 14412.713867\n",
      "554 loss 14403.089844\n",
      "555 loss 14393.543945\n",
      "556 loss 14384.077148\n",
      "557 loss 14374.688477\n",
      "558 loss 14365.375000\n",
      "559 loss 14356.139648\n",
      "560 loss 14346.980469\n",
      "561 loss 14337.897461\n",
      "562 loss 14328.887695\n",
      "563 loss 14319.953125\n",
      "564 loss 14311.094727\n",
      "565 loss 14302.305664\n",
      "566 loss 14293.590820\n",
      "567 loss 14284.952148\n",
      "568 loss 14276.382812\n",
      "569 loss 14267.886719\n",
      "570 loss 14259.459961\n",
      "571 loss 14251.102539\n",
      "572 loss 14242.818359\n",
      "573 loss 14234.602539\n",
      "574 loss 14226.453125\n",
      "575 loss 14218.375000\n",
      "576 loss 14210.365234\n",
      "577 loss 14202.420898\n",
      "578 loss 14194.546875\n",
      "579 loss 14186.736328\n",
      "580 loss 14178.994141\n",
      "581 loss 14171.315430\n",
      "582 loss 14163.702148\n",
      "583 loss 14156.156250\n",
      "584 loss 14148.671875\n",
      "585 loss 14141.251953\n",
      "586 loss 14133.897461\n",
      "587 loss 14126.605469\n",
      "588 loss 14119.374023\n",
      "589 loss 14112.204102\n",
      "590 loss 14105.096680\n",
      "591 loss 14098.050781\n",
      "592 loss 14091.065430\n",
      "593 loss 14084.140625\n",
      "594 loss 14077.275391\n",
      "595 loss 14070.468750\n",
      "596 loss 14063.721680\n",
      "597 loss 14057.033203\n",
      "598 loss 14050.402344\n",
      "599 loss 14043.830078\n",
      "600 loss 14037.311523\n",
      "601 loss 14030.855469\n",
      "602 loss 14024.452148\n",
      "603 loss 14018.103516\n",
      "604 loss 14011.811523\n",
      "605 loss 14005.576172\n",
      "606 loss 13999.394531\n",
      "607 loss 13993.266602\n",
      "608 loss 13987.195312\n",
      "609 loss 13981.174805\n",
      "610 loss 13975.208008\n",
      "611 loss 13969.291992\n",
      "612 loss 13963.430664\n",
      "613 loss 13957.620117\n",
      "614 loss 13951.862305\n",
      "615 loss 13946.153320\n",
      "616 loss 13940.497070\n",
      "617 loss 13934.889648\n",
      "618 loss 13929.333984\n",
      "619 loss 13923.826172\n",
      "620 loss 13918.370117\n",
      "621 loss 13912.958984\n",
      "622 loss 13907.598633\n",
      "623 loss 13902.288086\n",
      "624 loss 13897.022461\n",
      "625 loss 13891.803711\n",
      "626 loss 13886.631836\n",
      "627 loss 13881.508789\n",
      "628 loss 13876.430664\n",
      "629 loss 13871.400391\n",
      "630 loss 13866.411133\n",
      "631 loss 13861.469727\n",
      "632 loss 13856.574219\n",
      "633 loss 13851.721680\n",
      "634 loss 13846.915039\n",
      "635 loss 13842.149414\n",
      "636 loss 13837.428711\n",
      "637 loss 13832.751953\n",
      "638 loss 13828.116211\n",
      "639 loss 13823.523438\n",
      "640 loss 13818.973633\n",
      "641 loss 13814.463867\n",
      "642 loss 13809.997070\n",
      "643 loss 13805.570312\n",
      "644 loss 13801.184570\n",
      "645 loss 13796.838867\n",
      "646 loss 13792.535156\n",
      "647 loss 13788.269531\n",
      "648 loss 13784.043945\n",
      "649 loss 13779.857422\n",
      "650 loss 13775.708984\n",
      "651 loss 13771.600586\n",
      "652 loss 13767.530273\n",
      "653 loss 13763.497070\n",
      "654 loss 13759.500977\n",
      "655 loss 13755.542969\n",
      "656 loss 13751.622070\n",
      "657 loss 13747.738281\n",
      "658 loss 13743.887695\n",
      "659 loss 13740.077148\n",
      "660 loss 13736.299805\n",
      "661 loss 13732.559570\n",
      "662 loss 13728.855469\n",
      "663 loss 13725.183594\n",
      "664 loss 13721.548828\n",
      "665 loss 13717.946289\n",
      "666 loss 13714.379883\n",
      "667 loss 13710.845703\n",
      "668 loss 13707.345703\n",
      "669 loss 13703.877930\n",
      "670 loss 13700.445312\n",
      "671 loss 13697.042969\n",
      "672 loss 13693.672852\n",
      "673 loss 13690.338867\n",
      "674 loss 13687.034180\n",
      "675 loss 13683.759766\n",
      "676 loss 13680.517578\n",
      "677 loss 13677.307617\n",
      "678 loss 13674.128906\n",
      "679 loss 13670.980469\n",
      "680 loss 13667.861328\n",
      "681 loss 13664.772461\n",
      "682 loss 13661.711914\n",
      "683 loss 13658.681641\n",
      "684 loss 13655.681641\n",
      "685 loss 13652.711914\n",
      "686 loss 13649.767578\n",
      "687 loss 13646.853516\n",
      "688 loss 13643.969727\n",
      "689 loss 13641.112305\n",
      "690 loss 13638.284180\n",
      "691 loss 13635.482422\n",
      "692 loss 13632.706055\n",
      "693 loss 13629.960938\n",
      "694 loss 13627.239258\n",
      "695 loss 13624.544922\n",
      "696 loss 13621.878906\n",
      "697 loss 13619.236328\n",
      "698 loss 13616.621094\n",
      "699 loss 13614.032227\n",
      "700 loss 13611.467773\n",
      "701 loss 13608.929688\n",
      "702 loss 13606.416016\n",
      "703 loss 13603.927734\n",
      "704 loss 13601.462891\n",
      "705 loss 13599.024414\n",
      "706 loss 13596.609375\n",
      "707 loss 13594.217773\n",
      "708 loss 13591.849609\n",
      "709 loss 13589.506836\n",
      "710 loss 13587.185547\n",
      "711 loss 13584.889648\n",
      "712 loss 13582.615234\n",
      "713 loss 13580.362305\n",
      "714 loss 13578.133789\n",
      "715 loss 13575.927734\n",
      "716 loss 13573.744141\n",
      "717 loss 13571.582031\n",
      "718 loss 13569.441406\n",
      "719 loss 13567.322266\n",
      "720 loss 13565.223633\n",
      "721 loss 13563.148438\n",
      "722 loss 13561.091797\n",
      "723 loss 13559.057617\n",
      "724 loss 13557.043945\n",
      "725 loss 13555.049805\n",
      "726 loss 13553.077148\n",
      "727 loss 13551.124023\n",
      "728 loss 13549.191406\n",
      "729 loss 13547.278320\n",
      "730 loss 13545.384766\n",
      "731 loss 13543.508789\n",
      "732 loss 13541.654297\n",
      "733 loss 13539.817383\n",
      "734 loss 13538.000000\n",
      "735 loss 13536.201172\n",
      "736 loss 13534.420898\n",
      "737 loss 13532.659180\n",
      "738 loss 13530.916016\n",
      "739 loss 13529.190430\n",
      "740 loss 13527.482422\n",
      "741 loss 13525.792969\n",
      "742 loss 13524.119141\n",
      "743 loss 13522.462891\n",
      "744 loss 13520.825195\n",
      "745 loss 13519.206055\n",
      "746 loss 13517.600586\n",
      "747 loss 13516.012695\n",
      "748 loss 13514.444336\n",
      "749 loss 13512.887695\n",
      "750 loss 13511.348633\n",
      "751 loss 13509.828125\n",
      "752 loss 13508.320312\n",
      "753 loss 13506.829102\n",
      "754 loss 13505.356445\n",
      "755 loss 13503.896484\n",
      "756 loss 13502.452148\n",
      "757 loss 13501.022461\n",
      "758 loss 13499.609375\n",
      "759 loss 13498.208984\n",
      "760 loss 13496.826172\n",
      "761 loss 13495.457031\n",
      "762 loss 13494.101562\n",
      "763 loss 13492.761719\n",
      "764 loss 13491.435547\n",
      "765 loss 13490.124023\n",
      "766 loss 13488.825195\n",
      "767 loss 13487.541016\n",
      "768 loss 13486.271484\n",
      "769 loss 13485.014648\n",
      "770 loss 13483.769531\n",
      "771 loss 13482.540039\n",
      "772 loss 13481.324219\n",
      "773 loss 13480.118164\n",
      "774 loss 13478.927734\n",
      "775 loss 13477.750000\n",
      "776 loss 13476.583984\n",
      "777 loss 13475.431641\n",
      "778 loss 13474.291016\n",
      "779 loss 13473.163086\n",
      "780 loss 13472.045898\n",
      "781 loss 13470.942383\n",
      "782 loss 13469.850586\n",
      "783 loss 13468.771484\n",
      "784 loss 13467.701172\n",
      "785 loss 13466.644531\n",
      "786 loss 13465.599609\n",
      "787 loss 13464.566406\n",
      "788 loss 13463.542969\n",
      "789 loss 13462.532227\n",
      "790 loss 13461.531250\n",
      "791 loss 13460.542969\n",
      "792 loss 13459.563477\n",
      "793 loss 13458.595703\n",
      "794 loss 13457.637695\n",
      "795 loss 13456.692383\n",
      "796 loss 13455.754883\n",
      "797 loss 13454.830078\n",
      "798 loss 13453.914062\n",
      "799 loss 13453.006836\n",
      "800 loss 13452.112305\n",
      "801 loss 13451.224609\n",
      "802 loss 13450.348633\n",
      "803 loss 13449.485352\n",
      "804 loss 13448.626953\n",
      "805 loss 13447.782227\n",
      "806 loss 13446.944336\n",
      "807 loss 13446.115234\n",
      "808 loss 13445.295898\n",
      "809 loss 13444.486328\n",
      "810 loss 13443.684570\n",
      "811 loss 13442.893555\n",
      "812 loss 13442.110352\n",
      "813 loss 13441.335938\n",
      "814 loss 13440.571289\n",
      "815 loss 13439.813477\n",
      "816 loss 13439.064453\n",
      "817 loss 13438.326172\n",
      "818 loss 13437.591797\n",
      "819 loss 13436.869141\n",
      "820 loss 13436.155273\n",
      "821 loss 13435.447266\n",
      "822 loss 13434.749023\n",
      "823 loss 13434.057617\n",
      "824 loss 13433.373047\n",
      "825 loss 13432.698242\n",
      "826 loss 13432.031250\n",
      "827 loss 13431.369141\n",
      "828 loss 13430.717773\n",
      "829 loss 13430.072266\n",
      "830 loss 13429.433594\n",
      "831 loss 13428.801758\n",
      "832 loss 13428.178711\n",
      "833 loss 13427.563477\n",
      "834 loss 13426.952148\n",
      "835 loss 13426.351562\n",
      "836 loss 13425.755859\n",
      "837 loss 13425.167969\n",
      "838 loss 13424.585938\n",
      "839 loss 13424.010742\n",
      "840 loss 13423.443359\n",
      "841 loss 13422.879883\n",
      "842 loss 13422.325195\n",
      "843 loss 13421.777344\n",
      "844 loss 13421.233398\n",
      "845 loss 13420.697266\n",
      "846 loss 13420.166992\n",
      "847 loss 13419.643555\n",
      "848 loss 13419.125977\n",
      "849 loss 13418.613281\n",
      "850 loss 13418.108398\n",
      "851 loss 13417.608398\n",
      "852 loss 13417.114258\n",
      "853 loss 13416.625977\n",
      "854 loss 13416.142578\n",
      "855 loss 13415.666992\n",
      "856 loss 13415.195312\n",
      "857 loss 13414.731445\n",
      "858 loss 13414.269531\n",
      "859 loss 13413.815430\n",
      "860 loss 13413.364258\n",
      "861 loss 13412.921875\n",
      "862 loss 13412.483398\n",
      "863 loss 13412.047852\n",
      "864 loss 13411.619141\n",
      "865 loss 13411.195312\n",
      "866 loss 13410.776367\n",
      "867 loss 13410.364258\n",
      "868 loss 13409.955078\n",
      "869 loss 13409.551758\n",
      "870 loss 13409.152344\n",
      "871 loss 13408.757812\n",
      "872 loss 13408.368164\n",
      "873 loss 13407.985352\n",
      "874 loss 13407.602539\n",
      "875 loss 13407.228516\n",
      "876 loss 13406.856445\n",
      "877 loss 13406.490234\n",
      "878 loss 13406.126953\n",
      "879 loss 13405.768555\n",
      "880 loss 13405.416016\n",
      "881 loss 13405.065430\n",
      "882 loss 13404.720703\n",
      "883 loss 13404.379883\n",
      "884 loss 13404.041992\n",
      "885 loss 13403.709961\n",
      "886 loss 13403.380859\n",
      "887 loss 13403.055664\n",
      "888 loss 13402.735352\n",
      "889 loss 13402.417969\n",
      "890 loss 13402.105469\n",
      "891 loss 13401.795898\n",
      "892 loss 13401.490234\n",
      "893 loss 13401.189453\n",
      "894 loss 13400.888672\n",
      "895 loss 13400.594727\n",
      "896 loss 13400.304688\n",
      "897 loss 13400.016602\n",
      "898 loss 13399.734375\n",
      "899 loss 13399.453125\n",
      "900 loss 13399.175781\n",
      "901 loss 13398.902344\n",
      "902 loss 13398.631836\n",
      "903 loss 13398.366211\n",
      "904 loss 13398.101562\n",
      "905 loss 13397.840820\n",
      "906 loss 13397.584961\n",
      "907 loss 13397.331055\n",
      "908 loss 13397.080078\n",
      "909 loss 13396.833008\n",
      "910 loss 13396.587891\n",
      "911 loss 13396.346680\n",
      "912 loss 13396.106445\n",
      "913 loss 13395.871094\n",
      "914 loss 13395.638672\n",
      "915 loss 13395.410156\n",
      "916 loss 13395.182617\n",
      "917 loss 13394.958008\n",
      "918 loss 13394.736328\n",
      "919 loss 13394.517578\n",
      "920 loss 13394.302734\n",
      "921 loss 13394.088867\n",
      "922 loss 13393.879883\n",
      "923 loss 13393.670898\n",
      "924 loss 13393.466797\n",
      "925 loss 13393.262695\n",
      "926 loss 13393.061523\n",
      "927 loss 13392.865234\n",
      "928 loss 13392.671875\n",
      "929 loss 13392.478516\n",
      "930 loss 13392.288086\n",
      "931 loss 13392.100586\n",
      "932 loss 13391.915039\n",
      "933 loss 13391.733398\n",
      "934 loss 13391.551758\n",
      "935 loss 13391.374023\n",
      "936 loss 13391.196289\n",
      "937 loss 13391.023438\n",
      "938 loss 13390.851562\n",
      "939 loss 13390.681641\n",
      "940 loss 13390.514648\n",
      "941 loss 13390.349609\n",
      "942 loss 13390.188477\n",
      "943 loss 13390.026367\n",
      "944 loss 13389.867188\n",
      "945 loss 13389.708984\n",
      "946 loss 13389.555664\n",
      "947 loss 13389.403320\n",
      "948 loss 13389.251953\n",
      "949 loss 13389.102539\n",
      "950 loss 13388.956055\n",
      "951 loss 13388.809570\n",
      "952 loss 13388.666992\n",
      "953 loss 13388.526367\n",
      "954 loss 13388.385742\n",
      "955 loss 13388.250000\n",
      "956 loss 13388.113281\n",
      "957 loss 13387.977539\n",
      "958 loss 13387.845703\n",
      "959 loss 13387.714844\n",
      "960 loss 13387.585938\n",
      "961 loss 13387.459961\n",
      "962 loss 13387.332031\n",
      "963 loss 13387.208008\n",
      "964 loss 13387.085938\n",
      "965 loss 13386.966797\n",
      "966 loss 13386.845703\n",
      "967 loss 13386.728516\n",
      "968 loss 13386.613281\n",
      "969 loss 13386.497070\n",
      "970 loss 13386.385742\n",
      "971 loss 13386.272461\n",
      "972 loss 13386.163086\n",
      "973 loss 13386.053711\n",
      "974 loss 13385.946289\n",
      "975 loss 13385.839844\n",
      "976 loss 13385.737305\n",
      "977 loss 13385.631836\n",
      "978 loss 13385.533203\n",
      "979 loss 13385.431641\n",
      "980 loss 13385.333008\n",
      "981 loss 13385.235352\n",
      "982 loss 13385.139648\n",
      "983 loss 13385.043945\n",
      "984 loss 13384.950195\n",
      "985 loss 13384.858398\n",
      "986 loss 13384.764648\n",
      "987 loss 13384.675781\n",
      "988 loss 13384.586914\n",
      "989 loss 13384.500000\n",
      "990 loss 13384.412109\n",
      "991 loss 13384.326172\n",
      "992 loss 13384.242188\n",
      "993 loss 13384.160156\n",
      "994 loss 13384.077148\n",
      "995 loss 13383.997070\n",
      "996 loss 13383.916992\n",
      "997 loss 13383.836914\n",
      "998 loss 13383.759766\n",
      "999 loss 13383.683594\n"
     ]
    }
   ],
   "source": [
    "# mnist = input_data.read_data_sets('/tmp/data', one_hot=True)\n",
    "\n",
    "# Placeholders for images and labels\n",
    "# The first dimension is the batch size\n",
    "# None means it can be of any length\n",
    "# TensorFlow will infer it later\n",
    "x = tf.placeholder(tf.float32, [None, NUM_PIXELS], name=\"pixels\")\n",
    "y_ = tf.placeholder(tf.float32, [None, NUM_RES], name=\"labels\")\n",
    "\n",
    "# Define the model\n",
    "W_1 = tf.get_variable(shape=[NUM_PIXELS, HIDDEN_1], initializer=tf.contrib.layers.xavier_initializer(), name='w_1', regularizer=tf.contrib.layers.l2_regularizer(0.8))\n",
    "b_1 = tf.get_variable(shape=[HIDDEN_1], initializer=tf.contrib.layers.xavier_initializer(), name='b_1', regularizer=tf.contrib.layers.l2_regularizer(0.8))\n",
    "y_1 = tf.nn.relu(tf.matmul(x, W_1) + b_1)\n",
    "\n",
    "W_2 = tf.get_variable(shape=[HIDDEN_1, HIDDEN_2], initializer=tf.contrib.layers.xavier_initializer(), name='w_2', regularizer=tf.contrib.layers.l2_regularizer(0.8))\n",
    "b_2 = tf.get_variable(shape=[HIDDEN_2], initializer=tf.contrib.layers.xavier_initializer(), name='b_2', regularizer=tf.contrib.layers.l2_regularizer(0.8))\n",
    "y_2 = tf.nn.relu(tf.matmul(y_1, W_2) + b_2)\n",
    "\n",
    "W_3 = tf.get_variable(shape=[HIDDEN_2, NUM_RES], initializer=tf.contrib.layers.xavier_initializer(), name='w_3', regularizer=tf.contrib.layers.l2_regularizer(0.8))\n",
    "b_3 = tf.get_variable(shape=[NUM_RES], initializer=tf.contrib.layers.xavier_initializer(), name='b_3', regularizer=tf.contrib.layers.l2_regularizer(0.8))\n",
    "y = tf.matmul(y_2, W_3) + b_3\n",
    "\n",
    "# Write a summary of the graph (before we add the loss and optimizer)\n",
    "# Which will add a bunch of nodes automatically\n",
    "\n",
    "# Define loss and optimizer\n",
    "loss = tf.reduce_mean(tf.losses.mean_squared_error(predictions=y, labels=y_))\n",
    "train_step = tf.train.AdamOptimizer(LEARNING_RATE).minimize(loss)\n",
    "tf.summary.scalar('loss', loss)\n",
    "\n",
    "merged = tf.summary.merge_all()\n",
    "sw = tf.summary.FileWriter('summaries/', graph=tf.get_default_graph())\n",
    "\n",
    "# Initialize variables after the model is defined\n",
    "sess.run(tf.global_variables_initializer())\n",
    "\n",
    "# Train the model\n",
    "for i in range(TRAIN_STEPS):\n",
    "    sum_writer , _ , loss_val = sess.run([merged, train_step, loss], feed_dict={x: train_data, y_: train_labels})\n",
    "    sw.add_summary(sum_writer, i)\n",
    "#     if i % 100 == 0:\n",
    "    print (i, \"loss %f\" % loss_val)\n",
    "#         Weight = sess.run(W)\n",
    "#         for j in range(NUM_CLASSES):\n",
    "#             weight_0 = Weight[:, j]\n",
    "#             weight_0.shape = (400, 400)\n",
    "#             name = './weight_images/it_%d_class_%d.png' % (i, j)\n",
    "#             scipy.misc.imsave(name, weight_0)\n",
    "#             plt.imshow(weight_0, cmap=plt.cm.gray_r)\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "test_data = []\n",
    "test_labels = []\n",
    "for i in test_set:\n",
    "    regex_res = re.search('X([0-9]*\\.?[0-9]+)_Y([0-9]*\\.?[0-9]+)(_X([0-9]*\\.?[0-9]+)_Y([0-9]*\\.?[0-9]+))?.png', i)\n",
    "    \n",
    "    append_label = np.zeros((NUM_CLASSES), dtype=np.float32)\n",
    "    x_ind = (int(regex_res.group(1)) - 1)\n",
    "    np.put(append_label, x_ind, 1)\n",
    "    test_labels.append(append_label)\n",
    "    \n",
    "    img = scipy.misc.imread(i, flatten=False, mode='RGBA').astype(np.float)\n",
    "    img = img[:,:,3]\n",
    "    img.shape = (NUM_PIXELS)\n",
    "    img = img / MAX_VAL\n",
    "#     print (np.amin(img), np.amax(img))\n",
    "    test_data.append(img)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1),tf.argmax(y_, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "print(\"Accuracy %f\" % sess.run(accuracy, feed_dict={x: test_data,\n",
    "                                  y_: test_labels}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print (len(test_labels), len(test_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "prediction = tf.argmax(y, 1)\n",
    "\n",
    "def predict(idx):\n",
    "    image = test_data[idx]\n",
    "    return sess.run(prediction, feed_dict={x: [image]})\n",
    "\n",
    "idx = 98\n",
    "actual = np.argmax(test_labels[idx])\n",
    "print (\"Predicted: %d, Actual: %d\" % (predict(idx), actual))\n",
    "output = (test_data[idx].reshape((400,400)) * 255)\n",
    "plt.imshow(output, cmap=plt.cm.gray_r)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()\n",
    "saver.save(sess, './my_model/model', global_step=TRAIN_STEPS)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
